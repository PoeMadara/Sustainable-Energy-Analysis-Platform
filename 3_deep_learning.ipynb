{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Cargar tus datos\n",
    "data = pd.read_csv('data/clean/cleaned_data_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo las columnas más relevantes\n",
    "columns_to_keep = [\n",
    "    'solar_electricity', 'wind_electricity', 'biofuel_electricity', 'hydro_electricity',\n",
    "    'population', 'gdp', 'electricity_demand', 'energy_per_capita', 'energy_per_gdp',\n",
    "    'solar_consumption', 'wind_consumption', 'biofuel_consumption', 'hydro_consumption',\n",
    "    'country', 'iso_code', 'year'\n",
    "]\n",
    "\n",
    "# Filtrar el DataFrame para mantener solo las columnas seleccionadas\n",
    "df_filtered = data[columns_to_keep]\n",
    "\n",
    "# Manejar valores faltantes (opcional, según tus datos)\n",
    "df_filtered = df_filtered.dropna()  # O puedes usar imputación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar el codificador\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Aplicar Label Encoding a las columnas 'country' e 'iso_code' en df_filtered\n",
    "df_filtered['country'] = label_encoder.fit_transform(df_filtered['country'])\n",
    "df_filtered['iso_code'] = label_encoder.fit_transform(df_filtered['iso_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.drop(columns=['solar_electricity', 'wind_electricity', \n",
    "                               'biofuel_electricity', 'hydro_electricity'])\n",
    "y = df_filtered[['solar_consumption', 'wind_consumption', \n",
    "                 'biofuel_consumption', 'hydro_consumption']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza los datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Ajustar y transformar\n",
    "X_test_scaled = scaler.transform(X_test)  # Solo transforma, no ajusta de nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Carlos\\anaconda3\\envs\\ml-dp\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo\n",
    "dl_model = Sequential()\n",
    "\n",
    "# Capa de entrada\n",
    "dl_model.add(Input(shape=(X.shape[1],)))  # Asegúrate de que la forma sea correcta\n",
    "\n",
    "# Agregar capas ocultas\n",
    "dl_model.add(Dense(128, activation='relu', input_shape=(16,), kernel_regularizer=l2(0.01)))  # Primera capa oculta y regularización L2\n",
    "dl_model.add(Dense(64, activation='relu'))   # Segunda capa oculta\n",
    "dl_model.add(Dense(32, activation='relu'))    # Tercera capa oculta\n",
    "dl_model.add(Dense(16, activation='relu'))    # Cuarta capa oculta\n",
    "\n",
    "# Capa de salida para las 4 variables de consumo\n",
    "dl_model.add(Dense(4, activation='linear'))  # 4 neuronas para regresión múltiple\n",
    "\n",
    "# Compilar el modelo\n",
    "dl_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la carpeta final_models si no existe\n",
    "os.makedirs('final_models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback para guardar el mejor modelo\n",
    "checkpoint_path = 'final_models/deep_learning_best_model.keras'  # Ruta donde se guardará el modelo\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',  # Monitorea la pérdida de validación\n",
    "    mode='min',          # Guarda el modelo si la pérdida disminuye\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m509/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 51726.2852\n",
      "Epoch 1: val_loss improved from inf to 5294.34033, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 50343.3086 - val_loss: 5294.3403\n",
      "Epoch 2/100\n",
      "\u001b[1m496/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5916.6836\n",
      "Epoch 2: val_loss improved from 5294.34033 to 3183.94897, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5841.9014 - val_loss: 3183.9490\n",
      "Epoch 3/100\n",
      "\u001b[1m524/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3567.6997\n",
      "Epoch 3: val_loss improved from 3183.94897 to 2016.90491, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3542.0166 - val_loss: 2016.9049\n",
      "Epoch 4/100\n",
      "\u001b[1m527/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1142.1700\n",
      "Epoch 4: val_loss improved from 2016.90491 to 549.62329, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1129.1494 - val_loss: 549.6233\n",
      "Epoch 5/100\n",
      "\u001b[1m540/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4181.1201\n",
      "Epoch 5: val_loss improved from 549.62329 to 400.59937, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4157.3193 - val_loss: 400.5994\n",
      "Epoch 6/100\n",
      "\u001b[1m540/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 455.7408\n",
      "Epoch 6: val_loss improved from 400.59937 to 294.86380, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 455.6621 - val_loss: 294.8638\n",
      "Epoch 7/100\n",
      "\u001b[1m532/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 632.5640\n",
      "Epoch 7: val_loss did not improve from 294.86380\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 644.6725 - val_loss: 4301.1177\n",
      "Epoch 8/100\n",
      "\u001b[1m516/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 691.0986\n",
      "Epoch 8: val_loss did not improve from 294.86380\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 686.4169 - val_loss: 337.2213\n",
      "Epoch 9/100\n",
      "\u001b[1m522/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 277.6708\n",
      "Epoch 9: val_loss improved from 294.86380 to 289.36801, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 280.5122 - val_loss: 289.3680\n",
      "Epoch 10/100\n",
      "\u001b[1m535/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 724.7606\n",
      "Epoch 10: val_loss did not improve from 289.36801\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 729.3316 - val_loss: 848.7299\n",
      "Epoch 11/100\n",
      "\u001b[1m527/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 369.0914\n",
      "Epoch 11: val_loss did not improve from 289.36801\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 371.5081 - val_loss: 368.1701\n",
      "Epoch 12/100\n",
      "\u001b[1m533/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 925.4072\n",
      "Epoch 12: val_loss improved from 289.36801 to 269.58777, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 922.0545 - val_loss: 269.5878\n",
      "Epoch 13/100\n",
      "\u001b[1m538/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 281.1226\n",
      "Epoch 13: val_loss improved from 269.58777 to 202.31490, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 281.7013 - val_loss: 202.3149\n",
      "Epoch 14/100\n",
      "\u001b[1m535/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 248.4418\n",
      "Epoch 14: val_loss did not improve from 202.31490\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 249.1975 - val_loss: 414.2952\n",
      "Epoch 15/100\n",
      "\u001b[1m515/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 3438.7759\n",
      "Epoch 15: val_loss did not improve from 202.31490\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3332.0452 - val_loss: 211.0796\n",
      "Epoch 16/100\n",
      "\u001b[1m502/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 191.7978\n",
      "Epoch 16: val_loss improved from 202.31490 to 157.72566, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 195.1126 - val_loss: 157.7257\n",
      "Epoch 17/100\n",
      "\u001b[1m503/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 180.7811\n",
      "Epoch 17: val_loss improved from 157.72566 to 134.95239, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 180.2756 - val_loss: 134.9524\n",
      "Epoch 18/100\n",
      "\u001b[1m522/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 163.8075\n",
      "Epoch 18: val_loss did not improve from 134.95239\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 164.3249 - val_loss: 190.8272\n",
      "Epoch 19/100\n",
      "\u001b[1m528/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 191.8218\n",
      "Epoch 19: val_loss did not improve from 134.95239\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 192.6341 - val_loss: 578.4005\n",
      "Epoch 20/100\n",
      "\u001b[1m530/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1378.2556\n",
      "Epoch 20: val_loss did not improve from 134.95239\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1369.0011 - val_loss: 236.4060\n",
      "Epoch 21/100\n",
      "\u001b[1m541/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 319.1629\n",
      "Epoch 21: val_loss did not improve from 134.95239\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 318.6616 - val_loss: 135.9458\n",
      "Epoch 22/100\n",
      "\u001b[1m527/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 450.5947\n",
      "Epoch 22: val_loss did not improve from 134.95239\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 447.6220 - val_loss: 951.6530\n",
      "Epoch 23/100\n",
      "\u001b[1m513/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 286.3309\n",
      "Epoch 23: val_loss did not improve from 134.95239\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 289.2382 - val_loss: 230.9368\n",
      "Epoch 24/100\n",
      "\u001b[1m516/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 190.1160\n",
      "Epoch 24: val_loss improved from 134.95239 to 98.08315, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 187.1403 - val_loss: 98.0832\n",
      "Epoch 25/100\n",
      "\u001b[1m544/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 204.7728\n",
      "Epoch 25: val_loss did not improve from 98.08315\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 205.1001 - val_loss: 110.3938\n",
      "Epoch 26/100\n",
      "\u001b[1m519/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 255.6266\n",
      "Epoch 26: val_loss did not improve from 98.08315\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 256.8583 - val_loss: 226.1190\n",
      "Epoch 27/100\n",
      "\u001b[1m507/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 194.2932\n",
      "Epoch 27: val_loss improved from 98.08315 to 69.50854, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 191.5864 - val_loss: 69.5085\n",
      "Epoch 28/100\n",
      "\u001b[1m532/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 899.3111\n",
      "Epoch 28: val_loss improved from 69.50854 to 65.62576, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 894.3972 - val_loss: 65.6258\n",
      "Epoch 29/100\n",
      "\u001b[1m534/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 98.0703\n",
      "Epoch 29: val_loss did not improve from 65.62576\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 97.6543 - val_loss: 100.0168\n",
      "Epoch 30/100\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 210.7539\n",
      "Epoch 30: val_loss improved from 65.62576 to 54.67531, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 210.6332 - val_loss: 54.6753\n",
      "Epoch 31/100\n",
      "\u001b[1m523/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 91.7794\n",
      "Epoch 31: val_loss did not improve from 54.67531\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 95.5122 - val_loss: 409.2567\n",
      "Epoch 32/100\n",
      "\u001b[1m498/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 383.5326 \n",
      "Epoch 32: val_loss did not improve from 54.67531\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 374.5749 - val_loss: 68.2484\n",
      "Epoch 33/100\n",
      "\u001b[1m496/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 104.9257\n",
      "Epoch 33: val_loss did not improve from 54.67531\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 114.0358 - val_loss: 71.9536\n",
      "Epoch 34/100\n",
      "\u001b[1m494/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 277.1158\n",
      "Epoch 34: val_loss did not improve from 54.67531\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 268.7699 - val_loss: 159.8022\n",
      "Epoch 35/100\n",
      "\u001b[1m515/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 87.1992\n",
      "Epoch 35: val_loss did not improve from 54.67531\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.8020 - val_loss: 455.2953\n",
      "Epoch 36/100\n",
      "\u001b[1m540/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 221.1744\n",
      "Epoch 36: val_loss did not improve from 54.67531\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 220.9482 - val_loss: 80.4749\n",
      "Epoch 37/100\n",
      "\u001b[1m499/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 280.8594\n",
      "Epoch 37: val_loss did not improve from 54.67531\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 285.5985 - val_loss: 103.3576\n",
      "Epoch 38/100\n",
      "\u001b[1m506/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 91.5491\n",
      "Epoch 38: val_loss improved from 54.67531 to 41.01649, saving model to final_models/deep_learning_best_model.keras\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 91.9790 - val_loss: 41.0165\n",
      "Epoch 39/100\n",
      "\u001b[1m515/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.9443\n",
      "Epoch 39: val_loss did not improve from 41.01649\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 78.8521 - val_loss: 114.3618\n",
      "Epoch 40/100\n",
      "\u001b[1m497/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 311.0484\n",
      "Epoch 40: val_loss did not improve from 41.01649\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 312.9027 - val_loss: 105.3521\n",
      "Epoch 41/100\n",
      "\u001b[1m502/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 677.3184\n",
      "Epoch 41: val_loss did not improve from 41.01649\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 662.8057 - val_loss: 61.0820\n",
      "Epoch 42/100\n",
      "\u001b[1m536/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 121.1906\n",
      "Epoch 42: val_loss did not improve from 41.01649\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 121.9762 - val_loss: 90.6652\n",
      "Epoch 43/100\n",
      "\u001b[1m512/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.5132\n",
      "Epoch 43: val_loss did not improve from 41.01649\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 72.5078 - val_loss: 46.6928\n",
      "Epoch 44/100\n",
      "\u001b[1m497/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 61.1003\n",
      "Epoch 44: val_loss did not improve from 41.01649\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 61.6126 - val_loss: 70.6831\n",
      "Epoch 45/100\n",
      "\u001b[1m517/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 133.0563\n",
      "Epoch 45: val_loss did not improve from 41.01649\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 148.0965 - val_loss: 112.8235\n",
      "Epoch 46/100\n",
      "\u001b[1m516/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 135.0707\n",
      "Epoch 46: val_loss did not improve from 41.01649\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 132.6743 - val_loss: 52.2327\n",
      "Epoch 47/100\n",
      "\u001b[1m537/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 98.4182\n",
      "Epoch 47: val_loss did not improve from 41.01649\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 101.9599 - val_loss: 475.7979\n",
      "Epoch 48/100\n",
      "\u001b[1m522/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 340.3542\n",
      "Epoch 48: val_loss did not improve from 41.01649\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 336.0786 - val_loss: 171.4852\n",
      "Epoch 48: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27edfa460d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early Stopping para detener el modelo si no mejora\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "# Entrenar el modelo de Deep Learning\n",
    "dl_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, \n",
    "             validation_data=(X_test_scaled, y_test), \n",
    "             callbacks=[model_checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "DL Model - MAE: 1.9719368045336743, RMSE: 13.074468209924293, R²: 0.991767942905426\n"
     ]
    }
   ],
   "source": [
    "# Predicciones con el modelo de Deep Learning\n",
    "dl_predictions = dl_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluar el modelo de Deep Learning\n",
    "dl_mae = mean_absolute_error(y_test, dl_predictions)\n",
    "dl_rmse = np.sqrt(mean_squared_error(y_test, dl_predictions))  # Cambiado para evitar advertencia\n",
    "dl_r2 = r2_score(y_test, dl_predictions)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(f\"DL Model - MAE: {dl_mae}, RMSE: {dl_rmse}, R²: {dl_r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Cross-Validation Results - MAE: 2.2356219488452522, RMSE: 10.395313912500553, R²: 0.9823119478795794\n"
     ]
    }
   ],
   "source": [
    "# Implementar la validación cruzada\n",
    "\n",
    "# Configuración de KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Almacenar métricas para cada pliegue\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "for train_index, val_index in kfold.split(X_train_scaled):\n",
    "    X_train_cv, X_val_cv = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_cv, y_val_cv = y_train.values[train_index], y_train.values[val_index]\n",
    "\n",
    "    # Definir el modelo\n",
    "    cv_model = Sequential()\n",
    "    cv_model.add(Input(shape=(X.shape[1],)))\n",
    "    cv_model.add(Dense(128, activation='relu'))\n",
    "    cv_model.add(Dense(64, activation='relu'))\n",
    "    cv_model.add(Dense(32, activation='relu'))\n",
    "    cv_model.add(Dense(16, activation='relu'))\n",
    "    cv_model.add(Dense(4, activation='linear'))\n",
    "\n",
    "    # Compilar el modelo\n",
    "    cv_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    cv_model.fit(X_train_cv, y_train_cv, epochs=50, batch_size=32, \n",
    "                 validation_data=(X_val_cv, y_val_cv), \n",
    "                 verbose=0)  # Suprime salida de entrenamiento para limpieza\n",
    "\n",
    "    # Predicciones\n",
    "    y_val_pred = cv_model.predict(X_val_cv)\n",
    "\n",
    "    # Calcular métricas\n",
    "    mae = mean_absolute_error(y_val_cv, y_val_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_cv, y_val_pred))\n",
    "    r2 = r2_score(y_val_cv, y_val_pred)\n",
    "\n",
    "    # Almacenar métricas\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "    r2_list.append(r2)\n",
    "\n",
    "# Imprimir las métricas promedio de validación cruzada\n",
    "print(f\"Cross-Validation Results - MAE: {np.mean(mae_list)}, RMSE: {np.mean(rmse_list)}, R²: {np.mean(r2_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacer predicciones\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
