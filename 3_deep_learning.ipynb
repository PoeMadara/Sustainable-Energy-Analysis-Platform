{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Cargar tus datos\n",
    "data = pd.read_csv('data/clean/cleaned_data_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo las columnas más relevantes\n",
    "columns_to_keep = [\n",
    "    'solar_electricity', 'wind_electricity', 'biofuel_electricity', 'hydro_electricity',\n",
    "    'population', 'gdp', 'electricity_demand', 'energy_per_capita', 'energy_per_gdp',\n",
    "    'solar_consumption', 'wind_consumption', 'biofuel_consumption', 'hydro_consumption',\n",
    "    'country', 'iso_code', 'year'\n",
    "]\n",
    "\n",
    "# Filtrar el DataFrame para mantener solo las columnas seleccionadas\n",
    "df_filtered = data[columns_to_keep]\n",
    "\n",
    "# Manejar valores faltantes (opcional, según tus datos)\n",
    "df_filtered = df_filtered.dropna()  # O puedes usar imputación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar el codificador\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Aplicar Label Encoding a las columnas 'country' e 'iso_code' en df_filtered\n",
    "df_filtered['country'] = label_encoder.fit_transform(df_filtered['country'])\n",
    "df_filtered['iso_code'] = label_encoder.fit_transform(df_filtered['iso_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.drop(columns=['solar_electricity', 'wind_electricity', \n",
    "                               'biofuel_electricity', 'hydro_electricity'])\n",
    "y = df_filtered[['solar_consumption', 'wind_consumption', \n",
    "                 'biofuel_consumption', 'hydro_consumption']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza los datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Ajustar y transformar\n",
    "X_test_scaled = scaler.transform(X_test)  # Solo transforma, no ajusta de nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo\n",
    "dl_model = Sequential()\n",
    "\n",
    "# Capa de entrada\n",
    "dl_model.add(Input(shape=(X.shape[1],)))  # Asegúrate de que la forma sea correcta\n",
    "\n",
    "# Agregar capas ocultas\n",
    "dl_model.add(Dense(128, activation='relu', input_shape=(16,), kernel_regularizer=l2(0.01)))  # Primera capa oculta y regularización L2\n",
    "dl_model.add(Dense(64, activation='relu'))   # Segunda capa oculta\n",
    "dl_model.add(Dense(32, activation='relu'))    # Tercera capa oculta\n",
    "dl_model.add(Dense(16, activation='relu'))    # Cuarta capa oculta\n",
    "\n",
    "# Capa de salida para las 4 variables de consumo\n",
    "dl_model.add(Dense(4, activation='linear'))  # 4 neuronas para regresión múltiple\n",
    "\n",
    "# Compilar el modelo\n",
    "dl_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la carpeta final_models si no existe\n",
    "os.makedirs('final_models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback para guardar el mejor modelo\n",
    "checkpoint_path = 'final_models/deep_learning_best_model.keras'  # Ruta donde se guardará el modelo\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',  # Monitorea la pérdida de validación\n",
    "    mode='min',          # Guarda el modelo si la pérdida disminuye\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping para detener el modelo si no mejora\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "# Entrenar el modelo de Deep Learning\n",
    "dl_model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, \n",
    "             validation_data=(X_test_scaled, y_test), \n",
    "             callbacks=[model_checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones con el modelo de Deep Learning\n",
    "dl_predictions = dl_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluar el modelo de Deep Learning\n",
    "dl_mae = mean_absolute_error(y_test, dl_predictions)\n",
    "dl_rmse = np.sqrt(mean_squared_error(y_test, dl_predictions))  # Cambiado para evitar advertencia\n",
    "dl_r2 = r2_score(y_test, dl_predictions)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(f\"DL Model - MAE: {dl_mae}, RMSE: {dl_rmse}, R²: {dl_r2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar la validación cruzada\n",
    "\n",
    "# Configuración de KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Almacenar métricas para cada pliegue\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "for train_index, val_index in kfold.split(X_train_scaled):\n",
    "    X_train_cv, X_val_cv = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_cv, y_val_cv = y_train.values[train_index], y_train.values[val_index]\n",
    "\n",
    "    # Definir el modelo\n",
    "    cv_model = Sequential()\n",
    "    cv_model.add(Input(shape=(X.shape[1],)))\n",
    "    cv_model.add(Dense(128, activation='relu'))\n",
    "    cv_model.add(Dense(64, activation='relu'))\n",
    "    cv_model.add(Dense(32, activation='relu'))\n",
    "    cv_model.add(Dense(16, activation='relu'))\n",
    "    cv_model.add(Dense(4, activation='linear'))\n",
    "\n",
    "    # Compilar el modelo\n",
    "    cv_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    cv_model.fit(X_train_cv, y_train_cv, epochs=50, batch_size=32, \n",
    "                 validation_data=(X_val_cv, y_val_cv), \n",
    "                 verbose=0)  # Suprime salida de entrenamiento para limpieza\n",
    "\n",
    "    # Predicciones\n",
    "    y_val_pred = cv_model.predict(X_val_cv)\n",
    "\n",
    "    # Calcular métricas\n",
    "    mae = mean_absolute_error(y_val_cv, y_val_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_cv, y_val_pred))\n",
    "    r2 = r2_score(y_val_cv, y_val_pred)\n",
    "\n",
    "    # Almacenar métricas\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "    r2_list.append(r2)\n",
    "\n",
    "# Imprimir las métricas promedio de validación cruzada\n",
    "print(f\"Cross-Validation Results - MAE: {np.mean(mae_list)}, RMSE: {np.mean(rmse_list)}, R²: {np.mean(r2_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacer predicciones\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
